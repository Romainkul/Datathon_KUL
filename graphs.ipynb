{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSfsazP8unW0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\alisa\\Downloads\\preprocessed_data - preprocessed_data.csv'  # Update with the correct file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Create a heatmap to visualize the missing data\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(data.isnull(), cbar=False, cmap='viridis', yticklabels=False, xticklabels=True)\n",
        "plt.title('Missing Data Heatmap')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Rows')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\alisa\\Downloads\\preprocessed_data - preprocessed_data.csv'  # Update with the correct file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split topics by commas and clean up the data\n",
        "data['dutch_topics_split'] = data['topics_dutch_text'].str.split(',')\n",
        "data['german_topics_split'] = data['topics_german_text'].str.split(',')\n",
        "data['french_topics_split'] = data['topics_french_text'].str.split(',')\n",
        "\n",
        "# Flatten the lists to create a list of individual topics\n",
        "dutch_topics = [item for sublist in data['dutch_topics_split'].dropna() for item in sublist]\n",
        "german_topics = [item for sublist in data['german_topics_split'].dropna() for item in sublist]\n",
        "french_topics = [item for sublist in data['french_topics_split'].dropna() for item in sublist]\n",
        "\n",
        "# Count the occurrences of each topic\n",
        "dutch_topic_counts = pd.Series(dutch_topics).value_counts()\n",
        "german_topic_counts = pd.Series(german_topics).value_counts()\n",
        "french_topic_counts = pd.Series(french_topics).value_counts()\n",
        "\n",
        "# Plot the distribution of topics for each language\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Dutch Topics\n",
        "plt.subplot(1, 3, 1)\n",
        "dutch_topic_counts.head(10).plot(kind='bar', color='lightblue')\n",
        "plt.title('Top Dutch Topics')\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# German Topics\n",
        "plt.subplot(1, 3, 2)\n",
        "german_topic_counts.head(10).plot(kind='bar', color='lightgreen')\n",
        "plt.title('Top German Topics')\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# French Topics\n",
        "plt.subplot(1, 3, 3)\n",
        "french_topic_counts.head(10).plot(kind='bar', color='salmon')\n",
        "plt.title('Top French Topics')\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Adjust layout for better display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kI1J28dbuxw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\alisa\\Downloads\\preprocessed_data - preprocessed_data.csv'  # Use raw string to avoid issues with backslashes\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to perform sentiment analysis using TextBlob\n",
        "def analyze_sentiment(text):\n",
        "    # TextBlob provides a sentiment polarity score; we classify based on the polarity\n",
        "    if pd.isna(text):\n",
        "        return 'Neutral'  # Handle NaN values as neutral\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply sentiment analysis to the texts in Dutch, German, and French\n",
        "data['dutch_sentiment'] = data['dutch_text'].apply(analyze_sentiment)\n",
        "data['german_sentiment'] = data['german_text'].apply(analyze_sentiment)\n",
        "data['french_sentiment'] = data['french_text'].apply(analyze_sentiment)\n",
        "\n",
        "# Count the occurrences of each sentiment in each language\n",
        "dutch_sentiment_counts = data['dutch_sentiment'].value_counts()\n",
        "german_sentiment_counts = data['german_sentiment'].value_counts()\n",
        "french_sentiment_counts = data['french_sentiment'].value_counts()\n",
        "\n",
        "# Plot the distribution of sentiments for each language\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Dutch Sentiment\n",
        "plt.subplot(1, 3, 1)\n",
        "dutch_sentiment_counts.plot(kind='bar', color='lightblue')\n",
        "plt.title('Sentiment Distribution in Dutch')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# German Sentiment\n",
        "plt.subplot(1, 3, 2)\n",
        "german_sentiment_counts.plot(kind='bar', color='lightgreen')\n",
        "plt.title('Sentiment Distribution in German')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# French Sentiment\n",
        "plt.subplot(1, 3, 3)\n",
        "french_sentiment_counts.plot(kind='bar', color='salmon')\n",
        "plt.title('Sentiment Distribution in French')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Adjust layout for better display\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aSykxmcvvDGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, PCA  # Make sure to import PCA\n",
        "import umap\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\alisa\\Downloads\\preprocessed_data - preprocessed_data.csv'  # Update with the correct file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine text data (Dutch, German, French) into one column for analysis\n",
        "all_text = data['dutch_text'].dropna().tolist() + data['german_text'].dropna().tolist() + data['french_text'].dropna().tolist()\n",
        "\n",
        "# Vectorize the text using CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "X = vectorizer.fit_transform(all_text)\n",
        "\n",
        "# Apply LDA for topic modeling\n",
        "n_topics = 5  # Number of topics\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# Get the topic with the highest probability for each document\n",
        "topic_assignments = lda.transform(X)\n",
        "dominant_topics = topic_assignments.argmax(axis=1)\n",
        "\n",
        "# Reduce the dimensionality of the embeddings to 50 dimensions using PCA before applying UMAP\n",
        "pca = PCA(n_components=50)\n",
        "reduced_embeddings = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Apply UMAP for 2D visualization\n",
        "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_result = umap_model.fit_transform(reduced_embeddings)\n",
        "\n",
        "# Plot the UMAP visualization with colors based on topic\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a scatter plot where each point is colored by its dominant topic\n",
        "for topic in range(n_topics):\n",
        "    indices = [i for i, label in enumerate(dominant_topics) if label == topic]\n",
        "    plt.scatter(umap_result[indices, 0], umap_result[indices, 1], label=f'Topic {topic+1}', alpha=0.5, s=50)\n",
        "\n",
        "plt.title('UMAP Visualization of Text Embeddings by Topic')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TOXlRv9MvP98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # Updated import for pyLDAvis\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\alisa\\Downloads\\preprocessed_data - preprocessed_data.csv'  # Update with the correct file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine text data (Dutch, German, French) into one column for analysis\n",
        "all_text = data['dutch_text'].dropna().tolist() + data['german_text'].dropna().tolist() + data['french_text'].dropna().tolist()\n",
        "\n",
        "# Vectorize the text using CountVectorizer (you can also use TfidfVectorizer for TF-IDF)\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=1000)  # Limit the number of features to avoid sparse matrix issues\n",
        "X = vectorizer.fit_transform(all_text)\n",
        "\n",
        "# Apply LDA for topic modeling\n",
        "n_topics = 5  # You can change the number of topics here\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# Display the top words for each topic\n",
        "for index, topic in enumerate(lda.components_):\n",
        "    print(f\"Topic {index + 1}:\")\n",
        "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])  # Top 10 words per topic\n",
        "    print()\n",
        "\n",
        "# Visualize the topics using pyLDAvis\n",
        "pyLDAvis.enable_notebook()  # Use in Jupyter Notebook (can be omitted if ru\n"
      ],
      "metadata": {
        "id": "Yn4q5U9Cvexk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\alisa\\Downloads\\preprocessed_data - preprocessed_data.csv'\n",
        " # Path to the uploaded file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to parse embedding string into numpy array\n",
        "def parse_embedding(embedding_str):\n",
        "    return np.fromstring(embedding_str.strip('[]'), sep=' ')\n",
        "\n",
        "# Extract embeddings for Dutch, German, and French and parse them into arrays\n",
        "dutch_embeddings = data['dutch_embedding'].dropna().apply(parse_embedding).tolist()\n",
        "german_embeddings = data['german_embedding'].dropna().apply(parse_embedding).tolist()\n",
        "french_embeddings = data['french_embedding'].dropna().apply(parse_embedding).tolist()\n",
        "\n",
        "# Ensure all embeddings are of the same length (find the maximum length)\n",
        "embedding_length = max(len(embedding) for embedding in dutch_embeddings + german_embeddings + french_embeddings)\n",
        "\n",
        "# Pad or truncate embeddings to the same length\n",
        "def adjust_embedding_length(embeddings, target_length):\n",
        "    return [np.pad(embedding, (0, target_length - len(embedding)), mode='constant') if len(embedding) < target_length else embedding[:target_length] for embedding in embeddings]\n",
        "\n",
        "dutch_embeddings_adjusted = adjust_embedding_length(dutch_embeddings, embedding_length)\n",
        "german_embeddings_adjusted = adjust_embedding_length(german_embeddings, embedding_length)\n",
        "french_embeddings_adjusted = adjust_embedding_length(french_embeddings, embedding_length)\n",
        "\n",
        "# Combine all embeddings into one array\n",
        "all_embeddings_adjusted = np.array(dutch_embeddings_adjusted + german_embeddings_adjusted + french_embeddings_adjusted)\n",
        "\n",
        "# Reduce the dimensionality of the embeddings to 50 dimensions using PCA before applying UMAP\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=50)\n",
        "reduced_embeddings = pca.fit_transform(all_embeddings_adjusted)\n",
        "\n",
        "# Apply UMAP for 2D visualization\n",
        "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_result = umap_model.fit_transform(reduced_embeddings)\n",
        "\n",
        "# Create a list of labels for each language (Dutch, German, French)\n",
        "labels = ['Dutch'] * len(dutch_embeddings_adjusted) + ['German'] * len(german_embeddings_adjusted) + ['French'] * len(french_embeddings_adjusted)\n",
        "\n",
        "# Plot the UMAP visualization with colors based on language\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a color map based on the labels\n",
        "colors = {'Dutch': 'blue', 'German': 'green', 'French': 'red'}\n",
        "\n",
        "# Plotting\n",
        "for language in set(labels):\n",
        "    indices = [i for i, label in enumerate(labels) if label == language]\n",
        "    plt.scatter(umap_result[indices, 0], umap_result[indices, 1], label=language, alpha=0.5, s=50)\n",
        "\n",
        "plt.title('UMAP Visualization of Text Embeddings by Language')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "gxBWm8gQvikk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}